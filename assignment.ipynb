{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e619230",
   "metadata": {},
   "source": [
    "## 最終課題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fb699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# スタートURL（トップページ）\n",
    "start_url = \"https://www.musashino-u.ac.jp/\"\n",
    "domain = urlparse(start_url).netloc\n",
    "\n",
    "# User-Agent設定（botとみなされないように）\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/118.0.5993.70 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# 結果を格納する辞書\n",
    "site_dict = {}\n",
    "\n",
    "# BFS（幅優先探索）で全ページを巡回\n",
    "queue = deque([start_url])\n",
    "visited = set()\n",
    "\n",
    "# 除外する拡張子（画像・PDFなど）\n",
    "exclude_ext = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".pdf\", \".zip\", \".mp4\", \".doc\", \".docx\")\n",
    "\n",
    "while queue:\n",
    "    url = queue.popleft()\n",
    "\n",
    "    if url in visited:\n",
    "        continue\n",
    "    visited.add(url)\n",
    "\n",
    "    # ファイル系URLは除外\n",
    "    if url.lower().endswith(exclude_ext):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        if res.status_code != 200:\n",
    "            continue\n",
    "        res.encoding = res.apparent_encoding\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # コメントアウト内のリンクを除去\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "\n",
    "        # タイトルを取得\n",
    "        title = soup.title.string.strip() if soup.title and soup.title.string else \"No Title\"\n",
    "        site_dict[url] = title\n",
    "        print(f\"✔ {url} → {title}\")\n",
    "\n",
    "        # 全<a>タグのリンクを収集\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            link = urljoin(url, a[\"href\"])  # 絶対URLに変換\n",
    "            parsed = urlparse(link)\n",
    "\n",
    "            # 同一ドメインのみ & 除外拡張子でないリンク\n",
    "            if parsed.netloc == domain and not link.lower().endswith(exclude_ext):\n",
    "                if link not in visited:\n",
    "                    queue.append(link)\n",
    "\n",
    "        # 負荷対策（アクセス間隔）\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching {url}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n=== 抽出完了 ===\")\n",
    "print(f\"総ページ数：{len(site_dict)}\")\n",
    "print(site_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
